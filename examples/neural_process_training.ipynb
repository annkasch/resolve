{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from resolve.utilities import utilities as utils\n",
    "from resolve.helpers import DataGeneration\n",
    "from resolve.helpers import Trainer, ModelsManager\n",
    "from resolve.helpers import AsymmetricFocalWithFPPenalty, log_prob, bce_with_logits\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import yaml\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the yaml settings file here\n",
    "path_to_settings = \"./binary-black-hole/\"\n",
    "with open(f\"{path_to_settings}/settings.yaml\", \"r\") as f:\n",
    "    config_file = yaml.safe_load(f)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "version = config_file[\"path_settings\"][\"version\"]\n",
    "path_out = f'{config_file[\"path_settings\"][\"path_out_model\"]}/model-{version}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = config_file[\"model_settings\"][\"network\"][\"model_used\"]\n",
    "network_config = config_file[\"model_settings\"][\"network\"][\"models\"][model_name]\n",
    "network_config[\"d_y\"] = utils.get_feature_and_label_size(config_file)[1]\n",
    "network_config[\"d_theta\"]  = len(config_file[\"simulation_settings\"][\"theta_labels\"])\n",
    "network_config[\"d_phi\"] = len(config_file[\"simulation_settings\"][\"phi_labels\"])\n",
    "\n",
    "manager = ModelsManager(network_config)\n",
    "model = manager.get_network(config_file[\"model_settings\"][\"network\"][\"model_used\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positives ratio  tensor(0.0073)\n",
      "theta mean:  [1.50423200e-02 4.99557795e+00 5.00162000e+02 4.98470000e+02]\n",
      "phi mean:  [3.09912000e-02 1.38451402e+01 7.49881529e+01 3.83905393e+01\n",
      " 1.93798800e-01 5.03367206e-01 1.08481965e+02 1.17347808e+01\n",
      " 1.09972412e+01 9.90653600e-01]\n"
     ]
    }
   ],
   "source": [
    "# load data:\n",
    "dataset_train = DataGeneration(mode = \"train\", \n",
    "                                config_file=config_file\n",
    "                                )\n",
    "\n",
    "dataset_train.set_dataset()\n",
    "\n",
    "if config_file[\"model_settings\"][\"train\"][\"dataset\"][\"use_feature_normalization\"] == \"zscore\":\n",
    "    print(\"theta mean: \", dataset_train.dataset._normalizer._get_scaler(\"theta\").mean_)\n",
    "    print(\"phi mean: \", dataset_train.dataset._normalizer._get_scaler(\"phi\").mean_)\n",
    "elif config_file[\"model_settings\"][\"train\"][\"dataset\"][\"use_feature_normalization\"] == \"minmax\":\n",
    "    print(\"theta mean: \", dataset_train.dataset._normalizer._get_scaler(\"theta\").data_range_)\n",
    "    print(\"phi mean: \", dataset_train.dataset._normalizer._get_scaler(\"phi\").data_range_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(f'mkdir -p {path_out}/model_{version}_tensorboard_logs')\n",
    "os.system(f'rm {path_out}/model_{version}_tensorboard_logs/events*')\n",
    "writer = SummaryWriter(log_dir=f'{path_out}/model_{version}_tensorboard_logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train 1/50: 100%|██████████| 3000/3000 [00:41<00:00, 73.15it/s, loss=0.6116]\n",
      "validate 1/50: 100%|██████████| 1000/1000 [00:09<00:00, 110.92it/s, loss=0.6213]\n",
      "train 2/50: 100%|██████████| 3000/3000 [00:39<00:00, 75.97it/s, loss=0.5972]\n",
      "validate 2/50: 100%|██████████| 1000/1000 [00:08<00:00, 112.21it/s, loss=0.6124]\n",
      "train 3/50: 100%|██████████| 3000/3000 [00:39<00:00, 75.41it/s, loss=0.5940]\n",
      "validate 3/50: 100%|██████████| 1000/1000 [00:09<00:00, 110.46it/s, loss=0.6186]\n",
      "train 4/50: 100%|██████████| 3000/3000 [00:39<00:00, 76.07it/s, loss=0.5922]\n",
      "validate 4/50: 100%|██████████| 1000/1000 [00:09<00:00, 110.25it/s, loss=0.6069]\n",
      "train 5/50: 100%|██████████| 3000/3000 [00:40<00:00, 74.85it/s, loss=0.5913]\n",
      "validate 5/50: 100%|██████████| 1000/1000 [00:08<00:00, 111.13it/s, loss=0.6080]\n",
      "train 6/50: 100%|██████████| 3000/3000 [00:39<00:00, 75.35it/s, loss=0.5905]\n",
      "validate 6/50: 100%|██████████| 1000/1000 [00:09<00:00, 109.70it/s, loss=0.6041]\n",
      "train 7/50: 100%|██████████| 3000/3000 [00:39<00:00, 76.70it/s, loss=0.5898]\n",
      "validate 7/50: 100%|██████████| 1000/1000 [00:08<00:00, 111.22it/s, loss=0.5970]\n",
      "train 8/50: 100%|██████████| 3000/3000 [00:39<00:00, 76.08it/s, loss=0.5890]\n",
      "validate 8/50: 100%|██████████| 1000/1000 [00:09<00:00, 107.56it/s, loss=0.6034]\n",
      "train 9/50: 100%|██████████| 3000/3000 [00:40<00:00, 73.21it/s, loss=0.5883]\n",
      "validate 9/50: 100%|██████████| 1000/1000 [00:09<00:00, 108.06it/s, loss=0.5997]\n",
      "train 10/50: 100%|██████████| 3000/3000 [00:39<00:00, 76.44it/s, loss=0.5878]\n",
      "validate 10/50: 100%|██████████| 1000/1000 [00:09<00:00, 106.69it/s, loss=0.5987]\n",
      "train 11/50: 100%|██████████| 3000/3000 [00:40<00:00, 73.61it/s, loss=0.5874]\n",
      "validate 11/50: 100%|██████████| 1000/1000 [00:09<00:00, 107.89it/s, loss=0.6034]\n",
      "train 12/50: 100%|██████████| 3000/3000 [00:38<00:00, 77.27it/s, loss=0.5870]\n",
      "validate 12/50: 100%|██████████| 1000/1000 [00:08<00:00, 118.52it/s, loss=0.5946]\n",
      "train 13/50: 100%|██████████| 3000/3000 [00:37<00:00, 80.62it/s, loss=0.5868]\n",
      "validate 13/50: 100%|██████████| 1000/1000 [00:08<00:00, 118.02it/s, loss=0.5973]\n",
      "train 14/50: 100%|██████████| 3000/3000 [00:37<00:00, 79.77it/s, loss=0.5864]\n",
      "validate 14/50: 100%|██████████| 1000/1000 [00:08<00:00, 116.11it/s, loss=0.6040]\n",
      "train 15/50: 100%|██████████| 3000/3000 [00:37<00:00, 80.17it/s, loss=0.5859]\n",
      "validate 15/50: 100%|██████████| 1000/1000 [00:08<00:00, 113.60it/s, loss=0.6028]\n",
      "train 16/50: 100%|██████████| 3000/3000 [00:37<00:00, 80.39it/s, loss=0.5856]\n",
      "validate 16/50: 100%|██████████| 1000/1000 [00:08<00:00, 115.50it/s, loss=0.5980]\n",
      "train 17/50: 100%|██████████| 3000/3000 [00:37<00:00, 80.16it/s, loss=0.5853]\n",
      "validate 17/50: 100%|██████████| 1000/1000 [00:08<00:00, 115.85it/s, loss=0.5933]\n",
      "train 18/50: 100%|██████████| 3000/3000 [00:37<00:00, 80.53it/s, loss=0.5850]\n",
      "validate 18/50: 100%|██████████| 1000/1000 [00:08<00:00, 115.66it/s, loss=0.5980]\n",
      "train 19/50: 100%|██████████| 3000/3000 [00:36<00:00, 82.11it/s, loss=0.5848]\n",
      "validate 19/50: 100%|██████████| 1000/1000 [00:08<00:00, 119.60it/s, loss=0.5964]\n",
      "train 20/50: 100%|██████████| 3000/3000 [00:38<00:00, 78.92it/s, loss=0.5846]\n",
      "validate 20/50: 100%|██████████| 1000/1000 [00:08<00:00, 115.17it/s, loss=0.5972]\n",
      "train 21/50: 100%|██████████| 3000/3000 [00:37<00:00, 80.21it/s, loss=0.5844]\n",
      "validate 21/50: 100%|██████████| 1000/1000 [00:08<00:00, 116.14it/s, loss=0.5902]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10min 55s, sys: 2min 24s, total: 13min 20s\n",
      "Wall time: 17min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "optimizer = optim.Adam(model.parameters(), lr=config_file[\"model_settings\"][\"train\"][\"learning_rate\"])\n",
    "\n",
    "# Instantiate the training wrapper for the first phase\n",
    "trainer = Trainer(model, dataset_train)\n",
    "\n",
    "trainer.epochs = config_file[\"model_settings\"][\"train\"][\"training_epochs\"]\n",
    "\n",
    "trainer.criterion = AsymmetricFocalWithFPPenalty(\n",
    "                dataset_train.dataset.positive_ratio_data if config_file[\"model_settings\"][\"train\"][\"loss\"][\"prior_pos\"]==True else 0.5,\n",
    "                alpha_pos=utils.get_nested(config_file, [\"model_settings\",\"train\",\"loss\",\"alpha_pos\"], 1.),\n",
    "                alpha_neg=utils.get_nested(config_file, [\"model_settings\",\"train\",\"loss\",\"alpha_neg\"], 1.),\n",
    "                gamma_pos=utils.get_nested(config_file, [\"model_settings\",\"train\",\"loss\",\"gamma_pos\"], 0.),\n",
    "                gamma_neg=utils.get_nested(config_file, [\"model_settings\",\"train\",\"loss\",\"gamma_neg\"], 0.),\n",
    "                lambda_fp=utils.get_nested(config_file, [\"model_settings\",\"train\",\"loss\",\"lambda_fp\"],0.),\n",
    "                tau_fp=utils.get_nested(config_file, [\"model_settings\",\"train\",\"loss\",\"tau_fp\"],0.),\n",
    "                weight_fp=utils.get_nested(config_file, [\"model_settings\",\"train\",\"loss\",\"weight_fp\"],0.),\n",
    "                weight_tp=utils.get_nested(config_file, [\"model_settings\",\"train\",\"loss\",\"weight_tp\"],0.),\n",
    "                reduction=utils.get_nested(config_file, [\"model_settings\",\"train\",\"loss\",\"reduction\"], \"mean\"),\n",
    "                base_loss_fn=globals()[utils.get_nested(config_file, [\"model_settings\",\"train\",\"loss\",\"base_loss_fn\"], \"bce_with_logits\")],\n",
    "            )\n",
    "\n",
    "# Train the model\n",
    "summary_train = trainer.fit(optimizer=optimizer, writer=writer, ckpt_dir=f\"{path_out}/checkpoints\", ckpt_name=f\"model_{version}_best.pt\",\n",
    "        monitor=\"pr_auc\", mode=\"max\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_score': 0.4160555982471675,\n",
       " 'monitor': 'pr_auc',\n",
       " 'mode': 'max',\n",
       " 'epochs_ran': 21}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': {'accuracy': 0.5014576666666667,\n",
       "  'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1': 0.0,\n",
       "  'roc_auc': nan,\n",
       "  'pr_auc': nan,\n",
       "  'loss': 0.5843694984912873},\n",
       " 'validate': {'accuracy': 0.958686,\n",
       "  'precision': 0.14846872217505644,\n",
       "  'recall': 0.974052438527374,\n",
       "  'f1': 0.2576634204190175,\n",
       "  'roc_auc': 0.9897304782553006,\n",
       "  'pr_auc': 0.40250800355603145,\n",
       "  'loss': 0.5901790767908096},\n",
       " 'best_model': {'best_score': 0.4160555982471675,\n",
       "  'monitor': 'pr_auc',\n",
       "  'mode': 'max',\n",
       "  'epochs_ran': 21}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Trainer.evaluate() got an unexpected keyword argument 'dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m _ = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtest\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Software/miniconda3/envs/resolve-py3.11/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: Trainer.evaluate() got an unexpected keyword argument 'dataset'"
     ]
    }
   ],
   "source": [
    "_ = trainer.evaluate(dataset=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.metrics[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.save(model.state_dict(), f'{path_out}/model_{version}_model.pth')\n",
    "with open(f'{path_out}/model_{version}_settings.yaml', \"w\") as f:\n",
    "    yaml.safe_dump(dataset_train.config_file, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{path_out}/model_{version}_metrics.json', \"w\") as f:\n",
    "    json.dump(trainer.metrics, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train.dataset.close()\n",
    "writer.close()\n",
    "utils.cleanup_workspace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Start TensorBoard\n",
    "\n",
    "Run this in terminal:\n",
    "\n",
    "tensorboard --logdir=\\<path to tensor board log dir\\> --host=0.0.0.0 --port=7007\n",
    "\n",
    "Open:\n",
    "\n",
    "http://localhost:7007/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check effiency performance & memory use\n",
    "\n",
    "for memory:\n",
    "    \n",
    "    from memory_profiler import profile\n",
    "\n",
    "    @profile\n",
    "    def your_function():\n",
    "\n",
    "> python -m memory_profiler your_script.py  > mem_profile.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> python -m cProfile -s tottime your_script.py\n",
    "> sudo py-spy top -- python your_script.py\n",
    " \n",
    "> sudo py-spy record -o loader.svg -- python your_script.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "resolve-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
